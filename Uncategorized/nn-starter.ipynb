{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'p2' from 'c:\\\\Users\\\\Eric\\\\EricZacharia\\\\02-CareerEducation\\\\02-School\\\\01-UChicago\\\\04-Spring2021\\\\MPCS53111-MachineLearning\\\\Homework\\\\hw4\\\\p2.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "import time\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import p2 as nn  \n",
    "importlib.reload(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Brief overview of your tasks in the accompanying Python script.  Please also see the outline in the homework instructions.\n",
    "- You need to complete the neural network by fill in codes where there is a #todo prompt.\n",
    "- **The architecture of the neural network you are building is a vanilla version of that of Pytorch/Tensorflow consisting of nodes of *Operation Classes*. Each *Operation Class* has a forward method (to calculate the operation result stored in self.value) and a backward method (to calculate the gradient of the operation w.r.t. the final loss function).**\n",
    "- You need to implement forward and backward methods for *Operation Classes* Mul, VDot, Sigmoid, Relu (not required), Softmax, Log. Add, Aref, and Accuracy has been implemented for you for reference\n",
    "- self.params is the list to store the trainable parameters (objects of Class Param). set_weights(weights) has been implemented for you where the provided weights and biases are converted to Param objects and stored in self.params. You need to study the code and implement <code>get_weights()</code> and <code>init_weights_with_xavier()</code>\n",
    "- self.components is the list to natively mimic the function of the computational graph. Helper functions nn_unary_op(op, x) and nn_binary_op(op, x, y) are provided to facilitate creating an operation and adding it to the computational graph. For example, instead of $a=b+c$, you should use <code>a = self.nn_binary_op(Add, Value(b), Value(c))</code>. Only in this way you can create an operation object <code>a</code> with <code>a.value</code> and <code>a.grad</code> to support the forward and backward method of the neural network.\n",
    "- Placeholder including <code>self.sample_placeholder</code>, <code>self.label_placeholder</code>, <code>self.pred_placeholder</code>, <code>self.loss_placeholder</code>, <code>self.accy_placeholder</code> are all empty vectors that will be assigned values when executing <code>forward</code> or <code>backward</code>. They facilitate the construcation of the computational graph. <code>self.sample_placeholder</code> is the input to the NN. We feed different examples by calling <code>self.sample_placeholder.set(X[i])</code> and <code>self.label_placeholder.set(y[i])</code>. <code>self.pred_placeholder</code>, <code>self.loss_placeholder</code>, <code>self.accy_placeholder</code> changes values in each iteration in <code>fit</code>.\n",
    "- <code>self.forward()</code> is provided for you where each operation object in self.components are evalued from the begining to the end.\n",
    "- <code>self.backward()</code> is provided for you where derivative of each operation object in self.components are calculated from the end (loss function) to the beginning.\n",
    "- You need to implement sgd_update_parameter.\n",
    "- You could implement gradient_estimate to debug.\n",
    "- A test function test_set_and_get_weights() has been provided for you to test <code>self.get_weights</code>. Feel free to create more test functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Below is the test code for get_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the test for set_weights and get_weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eric\\EricZacharia\\02-CareerEducation\\02-School\\01-UChicago\\04-Spring2021\\MPCS53111-MachineLearning\\Homework\\hw4\\p2.py:395: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  weights = np.array(weights)\n"
     ]
    }
   ],
   "source": [
    "# You should expect the following message\n",
    "nn.test_set_and_get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Below are test cases to help you debug the different operation Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "m1 = nn.InputValue(np.arange(12).reshape((3,4)))\n",
    "m2 = nn.InputValue(np.arange(12,24).reshape((3,4)))\n",
    "m3 = nn.InputValue(np.arange(24,36).reshape((3,4)))\n",
    "m4 = nn.InputValue(np.arange(36,48).reshape((3,4)))\n",
    "\n",
    "v1 = nn.InputValue(np.arange(3).reshape((3,)))\n",
    "v2 = nn.InputValue(np.arange(3,6).reshape((3,)))\n",
    "v3 = nn.InputValue(np.arange(6,9).reshape((3,)))\n",
    "v4 = nn.InputValue(np.arange(4).reshape((4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on VDot\n"
     ]
    }
   ],
   "source": [
    "# Test VDot\n",
    "x = nn.Mul(m1, m2)\n",
    "y = nn.VDot(v1, x)\n",
    "z = nn.Mul(y, v4)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "v1.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([384., 463., 548., 639.])\n",
    "ygrad = np.array([0., 1., 2., 3.])\n",
    "yagrad = np.array([ 204.,  700., 1388.])\n",
    "ybgrad = np.array([[0., 0., 0., 0.],\n",
    "                   [0., 1., 2., 3.],\n",
    "                   [0., 2., 4., 6.]])\n",
    "\n",
    "if not np.array_equal(y.value, yvalue):\n",
    "    raise FailTestError(\"y.value not equal to matrix product of x.value and v1.value\")\n",
    "if not np.array_equal(y.grad, ygrad):\n",
    "    raise FailTestError(\"gradient of y is incorrect\")\n",
    "if not np.array_equal(y.a.grad, yagrad):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "if not np.array_equal(y.b.grad, ybgrad):\n",
    "    raise FailTestError(\"gradient of b in y is incorrect\")\n",
    "print(\"Passed Test on VDot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Test Sigmoid\n",
    "x = nn.Add(v1, v2)\n",
    "# print(' \\n x value: \\n', x.value,'\\n x gradient: \\n',x.grad, '\\n v1 value: \\n', v1.value,'\\n v1 gradient: \\n',v1.grad, '\\n v2 value: \\n', v2.value,'\\n v2 gradient: \\n',v2.grad)\n",
    "y = nn.Sigmoid(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([0.95257413, 0.9933072 , 0.999089], dtype=np.float32)\n",
    "yagrad = np.array([0.2710599 , 0.04653623, 0.00728134], dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to sigmoid of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on Sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5. 7.]\n",
      "[6. 7. 8.]\n",
      "Passing this teset is optional.\n",
      "Passed Test on RELU\n"
     ]
    }
   ],
   "source": [
    "# Test RELU\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.RELU(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "print(x.value)\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([3., 5., 7.], dtype=np.float32)\n",
    "yagrad = np.array([6., 7., 8.], dtype=np.float32)\n",
    "\n",
    "print(\"Passing this teset is optional.\")\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to Relu of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on RELU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02938593 -0.0998238   0.12920937]\n",
      "Passed Test on SoftMax\n"
     ]
    }
   ],
   "source": [
    "# Test SoftMax\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.SoftMax(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([0.01587624, 0.11731043, 0.86681336], dtype=np.float32)\n",
    "yagrad = np.array([-0.02938593, -0.09982383,  0.12920949], dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to SoftMax of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on SoftMax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Log\n"
     ]
    }
   ],
   "source": [
    "# Test Log\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.Log(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([1.0986123, 1.609438 , 1.9459102], dtype=np.float32)\n",
    "yagrad = np.array([2.       , 1.4      , 1.1428572], dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to log of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# we will use train.csv for training and testing as it test.csv doesn't contain label\n",
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "train_data = data.iloc[:30000]  # 30000\n",
    "test_data = data.iloc[30000:]  # 12000\n",
    "\n",
    "pixel_columns = [f\"pixel{i}\" for i in range(784)]\n",
    "\n",
    "# normalize by dividing by 255 as the pixel ranges from 0 to 255\n",
    "train_x = train_data[pixel_columns].values.astype(nn.DATA_TYPE)/255\n",
    "train_y = train_data[\"label\"].values.astype(nn.DATA_TYPE)\n",
    "\n",
    "test_x = test_data[pixel_columns].values.astype(nn.DATA_TYPE)/255\n",
    "test_y = test_data[\"label\"].values.astype(nn.DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Applying to the MNIST dataset\n",
    "- You should use the MNIST dataset to test the neural network you build above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Debugging the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append <Aref> to the computational graph\n",
      "Append <Log> to the computational graph\n",
      "Append <Mul> to the computational graph\n",
      "Append <Accuracy> to the computational graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eric\\EricZacharia\\02-CareerEducation\\02-School\\01-UChicago\\04-Spring2021\\MPCS53111-MachineLearning\\Homework\\hw4\\p2.py:366: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.accy_placeholder = None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a813b88a7a4d6697b14cf1f0c82900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11164/2576142292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# when calling fit, a computational graph will be built first, you should expect the exact lines printed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Load the sample gradient for debugging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Eric\\EricZacharia\\02-CareerEducation\\02-School\\01-UChicago\\04-Spring2021\\MPCS53111-MachineLearning\\Homework\\hw4\\p2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, alpha, t)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[1;31m# evaluate on train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mavg_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m             print(\"Epoch %d: train loss = %.4f, accy = %.4f, [%.3f secs]\" % (\n\u001b[0;32m    486\u001b[0m                 epoch, avg_loss, avg_acc, time.time()-since))\n",
      "\u001b[1;32mc:\\Users\\Eric\\EricZacharia\\02-CareerEducation\\02-School\\01-UChicago\\04-Spring2021\\MPCS53111-MachineLearning\\Homework\\hw4\\p2.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_placeholder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_placeholder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccy_placeholder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[0mobjective\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_placeholder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Eric\\EricZacharia\\02-CareerEducation\\02-School\\01-UChicago\\04-Spring2021\\MPCS53111-MachineLearning\\Homework\\hw4\\p2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Eric\\EricZacharia\\02-CareerEducation\\02-School\\01-UChicago\\04-Spring2021\\MPCS53111-MachineLearning\\Homework\\hw4\\p2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         self.value = np.array([np.log(self.a.value[i])\n\u001b[1;32m--> 248\u001b[1;33m                                for i in range(len(self.a.value))], dtype=np.float32)\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "importlib.reload(nn)  # important line so that the changes you made on p2.py will be reflected without restarting the kernel\n",
    "# Here we use the 2-layer NN with 1 hidden layer, feel free to experiment on your own\n",
    "nodes_array = [784, 128, 10]\n",
    "model = nn.NN(nodes_array, \"sigmoid\")\n",
    "\n",
    "# You can use the provided sample weights for initialization to help debug\n",
    "with open(\"./data/sample_weights.pkl\", 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "model.set_weights(weights)\n",
    "\n",
    "# You can use the first 2 samples to test if the gradients are correct\n",
    "X = train_x[:2]\n",
    "y = train_y[:2]\n",
    "\n",
    "# when calling fit, a computational graph will be built first, you should expect the exact lines printed\n",
    "model.fit(X, y, alpha=0.01, t=1)\n",
    "\n",
    "# Load the sample gradient for debugging\n",
    "with open(\"./data/sample_gradient.pkl\", 'rb') as f:\n",
    "    sample_grad = pickle.load(f)\n",
    "\n",
    "# first layer's weight of shape (784, 128)\n",
    "if not np.array_equal(np.round(model.params[0].grad, 3), np.round(sample_grad[\"w1\"], 3)):\n",
    "    raise FailTestError(\"gradient of the first layer's weight is incorrect\")\n",
    "# first layer's bias of shape (128, )\n",
    "if not np.array_equal(np.round(model.params[1].grad, 3), np.round(sample_grad[\"b1\"], 3)):\n",
    "    raise FailTestError(\"gradient of the first layer's bias is incorrect\")\n",
    "# second layer's weight of shape (128, 10)\n",
    "if not np.array_equal(np.round(model.params[2].grad, 3), np.round(sample_grad[\"w2\"], 3)):\n",
    "    raise FailTestError(\"gradient of the second layer's weight is incorrect\")\n",
    "# second layer's bias of shape (10, )\n",
    "if not np.array_equal(np.round(model.params[3].grad, 3), np.round(sample_grad[\"b2\"], 3)):\n",
    "    raise FailTestError(\"gradient of the second layer's bias is incorrect\")\n",
    "print(\"Congrats! You have passed the test of your fit function, your NN model should be good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Now train your NN on the whole training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(nn)  # important line so that the changes you made on p2.py will be reflected without restarting the kernel\n",
    "# Here we use the 2-layer NN with 1 hidden layer, feel free to experiment on your own\n",
    "nodes_array = [784, 128, 10]\n",
    "model = nn.NN(nodes_array, \"sigmoid\")\n",
    "model.init_weights_with_xavier()\n",
    "model.fit(train_x, train_y, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# After 10 epochs of training, you should expect an accuracy over 95% and loss around 0.1\n",
    "accy, loss = model.eval(test_x, test_y)\n",
    "print(\"Test accuracy = %.4f, accy = %.4f\" % (accy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

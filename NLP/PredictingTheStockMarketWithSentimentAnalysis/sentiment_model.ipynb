{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy_of_08_sentiment_analysis_with_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "5d33e5817340280fbce9a5097025b46f44ef76649f095e4aa6c33427194546ee"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9268bc6049b44fecae5181350e9badd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68ae2c7435df4e5d8c49b4a3b1cbd265",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8166bfacefde414f8b937820d797027e",
              "IPY_MODEL_b5376b9fa5914206b21cbe2738fbafbc",
              "IPY_MODEL_82a2224353a74e289dea161736961ae4"
            ]
          }
        },
        "68ae2c7435df4e5d8c49b4a3b1cbd265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8166bfacefde414f8b937820d797027e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0acf538283b145ff939dda067159578c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_827c7875f86a437e8130b27e74527ec9"
          }
        },
        "b5376b9fa5914206b21cbe2738fbafbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c34beb1a07f148c7a256c5fa93d5f5ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab66a5cd967b412d9172bf4145f84dd9"
          }
        },
        "82a2224353a74e289dea161736961ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24ed565edfd3467fa86bfdf2f472ccf3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:09&lt;00:00, 50.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6fd458087d74003a9c8071b79f17d00"
          }
        },
        "0acf538283b145ff939dda067159578c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "827c7875f86a437e8130b27e74527ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c34beb1a07f148c7a256c5fa93d5f5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab66a5cd967b412d9172bf4145f84dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24ed565edfd3467fa86bfdf2f472ccf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6fd458087d74003a9c8071b79f17d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "while True:\r\n",
        "    PRETRAINED = input(f'''Would you like to use the last saved trained model (p) or overwrite it with a new training session (train_new_model)?\r\n",
        "p: pretrained, train_new_model: overwrite with new session''')\r\n",
        "    if PRETRAINED == 'p':\r\n",
        "        PRETRAINED = True\r\n",
        "        break\r\n",
        "    elif PRETRAINED == 'train_new_model':\r\n",
        "        PRETRAINED = False\r\n",
        "        break\r\n",
        "    else:\r\n",
        "        print(\"Invalid response. type p: pretrained, or train_new_model: overwrite with new session\")\r\n",
        "\r\n",
        "while True:\r\n",
        "    COLAB = input(f'''Type (gc) if you are using Google Colab with GPU or (local) if you are running on your local machine's CPU.''')\r\n",
        "    if COLAB == 'gc':\r\n",
        "        COLAB = True\r\n",
        "        break\r\n",
        "    elif COLAB == 'local':\r\n",
        "        COLAB = False\r\n",
        "        break\r\n",
        "    else:\r\n",
        "        print(\"Invalid response. type gc: Google Colab, or local: local machine\")\r\n",
        "\r\n",
        "RANDOM_SEED = 42\r\n",
        "RISK_LEVEL = 0.2  # (conservative) 0.0 to 1.0 (risky)\r\n",
        "!pip install -r requirements.txt"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report\r\n",
        "from collections import defaultdict\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torch.nn.functional as F\r\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "np.random.seed(RANDOM_SEED)\r\n",
        "torch.manual_seed(RANDOM_SEED)\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "path = './'\r\n",
        "if COLAB:\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount('/content/gdrive/')\r\n",
        "    path = '/content/gdrive/My Drive/Transformer/'  \r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "w68CZpOwFoly",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f618fad-fc1b-4ae7-ffca-8f17746ddb7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.read_csv(f\"{path}data/stock_data.csv\", sep=',')\r\n",
        "df.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "mUKLyKc7I6Qp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "ff02dd21-3cc7-4768-e4bb-9395350c3a5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.tail()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "x = {'a':['1551, 67909621, 964182623, DATE OF ADMISSION: 7/30/2020          DATE OF DISCHARGE: No discharge date for patient encounter.  SERVICE:  Cardiac Surgery Service      ATTENDING PHYSICIAN: Valluvan Jeevanandam, M.D.      ADMIT & DISCHARGE DIAGNOSIS  Active Problems:    Accelerated junctional rhythm (8/12/2020)      PROCEDURES PERFORMED  Procedures Performed During Hospitalization  Procedure(s):  EP DUAL CHAMBER PACEMAKER  EP ANESTHESIA RESOURCE  Surgeon Role  Surgeon(s) and Role:     * Gaurav A Upadhyay, M.D. - Primary  Preop Diagnosis Code  Pre-Op Diagnosis Codes:     * Mitral valve stenosis, unspecified etiology [I05.0]  Date of Procedure  8/12/2020  -------------------  **Canceled**  Procedures Performed During Hospitalization  Procedure(s):  EP DUAL CHAMBER PACEMAKER  EP ANESTHESIA RESOURCE  Surgeon Role  Surgeon(s) and Role:     * Gaurav A Upadhyay, M.D. - Primary  Preop Diagnosis Code  Pre-Op Diagnosis Codes:     * Bradycardia [R00.1]  Date of Procedure  8/12/2020  -------------------  **Canceled**  Procedures Performed During Hospitalization  Procedure(s):  EP DUAL CHAMBER PACEMAKER  EP ANESTHESIA RESOURCE  Surgeon Role  Surgeon(s) and Role:     * Gaurav A Upadhyay, M.D. - Primary  Preop Diagnosis Code  Pre-Op Diagnosis Codes:     * Bradycardia [R00.1]  Date of Procedure  8/12/2020  -------------------  HISTORY OF PRESENT ILLNESS  Rosalyn Harris is a 59Yrs old female with a PMH of rheumatic MS, moderate TR, Afib on eliquis, mild nonobstructive CAD, pulmonary HTN, CVA 4/2020 (acute right basal ganglia infarct with left hemiparesis and foor drop), rheumatoid arthritis on methotrexate, who presents for surgical evaluation and treatment of mitral stenosis. She had a TTE on 4/14/20 s/p stroke which noted EF 60-65%, mild MR, moderate to severe MS (mean grad 7.8, pk grad 14 MVA 1.34 and 0.76). She subsequently underwent a TEE with bubble study on 6/23/20 which demonstrated EF 55-60%, no ASD no PFO no thrombus in LAA, severe MS, mild MR.  DATE OF ADMISSION: 7/30/2020          DATE OF DISCHARGE: No discharge date for patient encounter.  SERVICE:  Cardiac Surgery Service      ATTENDING PHYSICIAN: Valluvan Jeevanandam, M.D.      ADMIT & DISCHARGE DIAGNOSIS  Active Problems:    Accelerated junctional rhythm (8/12/2020)      PROCEDURES PERFORMED  Procedures Performed During Hospitalization  Procedure(s):  EP DUAL CHAMBER PACEMAKER  EP ANESTHESIA RESOURCE  Surgeon Role  Surgeon(s) and Role:     * Gaurav A Upadhyay, M.D. - Primary  Preop Diagnosis Code  Pre-Op Diagnosis Codes:     * Mitral valve stenosis, unspecified etiology [I05.0]  Date of Procedure  8/12/2020  -------------------  **Canceled**  Procedures Performed During Hospitalization  Procedure(s):  EP DUAL CHAMBER PACEMAKER  EP ANESTHESIA RESOURCE  Surgeon Role  Surgeon(s) and Role:     * Gaurav A Upadhyay, M.D. - Primary  Preop Diagnosis Code  Pre-Op Diagnosis Codes:     * Bradycardia [R00.1]  Date of Procedure  8/12/2020  -------------------  **Canceled**  Procedures Performed During Hospitalization  Procedure(s):  EP DUAL CHAMBER PACEMAKER  EP ANESTHESIA RESOURCE  Surgeon Role  Surgeon(s) and Role:     * Gaurav A Upadhyay, M.D. - Primary  Preop Diagnosis Code  Pre-Op Diagnosis Codes:     * Bradycardia [R00.1]  Date of Procedure  8/12/2020  -------------------  HISTORY OF PRESENT ILLNESS  Rosalyn Harris is a 59Yrs old female with a PMH of rheumatic MS, moderate TR, Afib on eliquis, mild nonobstructive CAD, pulmonary HTN, CVA 4/2020 (acute right basal ganglia infarct with left hemiparesis and foor drop), rheumatoid arthritis on methotrexate, who presents for surgical evaluation and treatment of mitral stenosis. She had a TTE on 4/14/20 s/p stroke which noted EF 60-65%, mild MR, moderate to severe MS (mean grad 7.8, pk grad 14 MVA 1.34 and 0.76). She subsequently underwent a TEE with bubble study on 6/23/20 which demonstrated EF 55-60%, no ASD no PFO no thrombus in LAA, severe MS, mild MR.  Cath from 6/23/20 demonstrated LCx: distal 20% and 10% proximal stenosis otherwise, no significant obstructive disease, RA 6, RV 69/7, PA 70/26/46, PCWP 25 Pa Sat 62%, Td CO 3.23, CI 1.98. Today she has complaints of DOE which has been ongoing since prior to her stroke in April and has been stable since. The DOE is described as “winded”, located throughout the chest, and is not rated by severity. The patient states the DOE is associated with decreased exercise tolerance, fatigue and orthopnea (2 pillow).  Rosalyn Harris states that the DOE is improved following rest. The symptoms are aggravated by mild exetion such as walking more than 20feet or walking up a few steps. He/She admits to had similar symptoms in the past. She denies SOB at rest, orthopnea, chest pain, LE , dizziness, syncope.     DISCHARGE PHYSICAL EXAMINATION  BP 107/64  | Pulse 79  | Temp 36.9 °C (98.4 °F) (NCIT)  | Resp 20  | Ht 152.4 cm (5'], 'd' : [' | Wt 60.8 kg (134 lb 0.6 oz)  | SpO2 97%  | BMI 26.18 kg/m²   GENERAL:  Well-developed, well-nourished, observed sit']}\r\n",
        "df = pd.DataFrame.from_dict(x)\r\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   a  \\\n",
              "0  1551, 67909621, 964182623, DATE OF ADMISSION: ...   \n",
              "\n",
              "                                                   d  \n",
              "0   | Wt 60.8 kg (134 lb 0.6 oz)  | SpO2 97%  | B...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1551, 67909621, 964182623, DATE OF ADMISSION: ...</td>\n",
              "      <td>| Wt 60.8 kg (134 lb 0.6 oz)  | SpO2 97%  | B...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "df.replace([r'[^\\x00-\\x7F]', r'[\\d+]'],\r\n",
        "           ['', ''], regex=True, inplace=True)\r\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   a  \\\n",
              "0  ,,,DATEOFADMISSION://DATEOFDISCHARGE:Nodischar...   \n",
              "\n",
              "                                                   d  \n",
              "0  Wt.kg(lb.oz)SpO%BMI.kg/mGENERAL:Well-developed...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>,,,DATEOFADMISSION://DATEOFDISCHARGE:Nodischar...</td>\n",
              "      <td>Wt.kg(lb.oz)SpO%BMI.kg/mGENERAL:Well-developed...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.Sentiment.replace(-1,0,inplace=True)\r\n",
        "df.tail()"
      ],
      "outputs": [],
      "metadata": {
        "id": "t-zNXeDb9zmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class StockTweetsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, tweets, targets, tokenizer, max_len):\r\n",
        "        self.tweets = tweets\r\n",
        "        self.targets = targets\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_len = max_len\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.tweets)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        tweet = str(self.tweets[item])\r\n",
        "        target = self.targets[item]\r\n",
        "\r\n",
        "        encoding = self.tokenizer.encode_plus(\r\n",
        "            tweet,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=self.max_len,\r\n",
        "            return_token_type_ids=False,\r\n",
        "            pad_to_max_length=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt')\r\n",
        "\r\n",
        "        return {\r\n",
        "            'tweet_text': tweet,\r\n",
        "            'input_ids': encoding['input_ids'].flatten(),\r\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\r\n",
        "            'targets': torch.tensor(target, dtype=torch.long)}\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "TucjhIkz4Fir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Split 80-10-10\r\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\r\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\r\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "B-vWzoo81dvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\r\n",
        "    ds = StockTweetsDataset(tweets=df.Text.to_numpy(), targets=df.Sentiment.to_numpy(), tokenizer=tokenizer, max_len=max_len)\r\n",
        "    return DataLoader(ds, batch_size=batch_size, num_workers=4)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "BATCH_SIZE = 16\r\n",
        "MAX_LEN = 80  # All tweets in the data set contain fewer than 80 tokens\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', return_dict=False)\r\n",
        "\r\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\r\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\r\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vODDxMKsPHqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fb4bec-b59f-4239-f390-ac9b37ad0953"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class SentimentClassifier(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, n_classes):\r\n",
        "    super(SentimentClassifier, self).__init__()\r\n",
        "    self.bert = BertModel.from_pretrained('bert-base-cased', return_dict=False)\r\n",
        "    self.dropout = nn.Dropout(p=0.3)\r\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n",
        "  \r\n",
        "  def forward(self, input_ids, attention_mask):\r\n",
        "    _, output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\r\n",
        "    output = self.dropout(output)\r\n",
        "    return self.out(output)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "m_mRflxPl32F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class_names = ['bearish', 'bullish']\r\n",
        "\r\n",
        "if not PRETRAINED:\r\n",
        "    data = next(iter(train_data_loader))\r\n",
        "    data.keys()\r\n",
        "    print(data['input_ids'].shape)\r\n",
        "    print(data['attention_mask'].shape)\r\n",
        "    print(data['targets'].shape)\r\n",
        "    model = SentimentClassifier(len(class_names))\r\n",
        "    model = model.to(device)\r\n",
        "    input_ids = data['input_ids'].to(device)\r\n",
        "    attention_mask = data['attention_mask'].to(device)\r\n",
        "    F.softmax(model(input_ids, attention_mask), dim=1)\r\n",
        "    EPOCHS = 10\r\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\r\n",
        "    total_steps = len(train_data_loader) * EPOCHS\r\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\r\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "rlS2wK6M4pAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f9db28-ed9f-4893-c105-bda98d07ebe7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\r\n",
        "    model = model.train()\r\n",
        "    losses = []\r\n",
        "    correct_predictions = 0\r\n",
        "\r\n",
        "    for d in data_loader:\r\n",
        "        input_ids = d[\"input_ids\"].to(device)\r\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "        targets = d[\"targets\"].to(device)\r\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
        "        _, preds = torch.max(outputs, dim=1)\r\n",
        "        loss = loss_fn(outputs, targets)\r\n",
        "        correct_predictions += torch.sum(preds == targets)\r\n",
        "        losses.append(loss.item())\r\n",
        "        loss.backward()\r\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\r\n",
        "        optimizer.step()\r\n",
        "        scheduler.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\r\n",
        "    model = model.eval()\r\n",
        "    losses = []\r\n",
        "    correct_predictions = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for d in data_loader:\r\n",
        "            input_ids = d[\"input_ids\"].to(device)\r\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "            targets = d[\"targets\"].to(device)\r\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
        "            _, preds = torch.max(outputs, dim=1)\r\n",
        "            loss = loss_fn(outputs, targets)\r\n",
        "            correct_predictions += torch.sum(preds == targets)\r\n",
        "            losses.append(loss.item())\r\n",
        "\r\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "PATH = 'twitter_sentiment_model.pth'  # filename for the pretrained model\r\n",
        "\r\n",
        "if not PRETRAINED:\r\n",
        "    %%time\r\n",
        "    history = defaultdict(list)\r\n",
        "    best_accuracy = 0\r\n",
        "\r\n",
        "    for epoch in range(EPOCHS):\r\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\r\n",
        "        print('-' * 10)\r\n",
        "        train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\r\n",
        "        print(f'Train loss {train_loss} accuracy {train_acc}')\r\n",
        "        val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\r\n",
        "        print(f'Val   loss {val_loss} accuracy {val_acc}')\r\n",
        "        print()\r\n",
        "        history['train_acc'].append(train_acc)\r\n",
        "        history['train_loss'].append(train_loss)\r\n",
        "        history['val_acc'].append(val_acc)\r\n",
        "        history['val_loss'].append(val_loss)\r\n",
        "\r\n",
        "        if val_acc > best_accuracy:\r\n",
        "            device = torch.device(\"cuda\")\r\n",
        "            model.to(device)\r\n",
        "            torch.save({\r\n",
        "                'history': history,\r\n",
        "                'epoch': EPOCHS,\r\n",
        "                'model_state_dict': model.state_dict(),\r\n",
        "                'optimizer_state_dict': optimizer.state_dict()}, PATH)\r\n",
        "            best_accuracy = val_acc\r\n",
        "else:\r\n",
        "    model = SentimentClassifier(len(class_names))\r\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\r\n",
        "\r\n",
        "    if COLAB:\r\n",
        "        PATH = path + PATH\r\n",
        "        device = torch.device('cuda')\r\n",
        "        checkpoint = torch.load(PATH)\r\n",
        "        model.to(device)\r\n",
        "    else:\r\n",
        "        device = torch.device('cpu')\r\n",
        "        checkpoint = torch.load(PATH, map_location=device)\r\n",
        "\r\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "    epoch = checkpoint['epoch']\r\n",
        "    history = checkpoint['history']"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Plot training accuracy and validation accuracy\r\n",
        "plt.plot(history['train_acc'], label='train accuracy')\r\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\r\n",
        "plt.title('Training history')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend()\r\n",
        "plt.ylim([0, 1])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if not PRETRAINED:\r\n",
        "    test_acc, _ = eval_model(model, test_data_loader, loss_fn, device, len(df_test))\r\n",
        "    test_acc.item()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_predictions(model, data_loader):\r\n",
        "    model = model.eval()\r\n",
        "    tweets = []\r\n",
        "    predictions = []\r\n",
        "    prediction_probs = []\r\n",
        "    real_values = []\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for d in data_loader:\r\n",
        "            texts = d[\"tweet_text\"]\r\n",
        "            input_ids = d[\"input_ids\"].to(device)\r\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "            targets = d[\"targets\"].to(device)\r\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
        "            _, preds = torch.max(outputs, dim=1)\r\n",
        "            probs = F.softmax(outputs, dim=1)\r\n",
        "            tweets.extend(texts)\r\n",
        "            predictions.extend(preds)\r\n",
        "            prediction_probs.extend(probs)\r\n",
        "            real_values.extend(targets)\r\n",
        "\r\n",
        "    predictions = torch.stack(predictions).cpu()\r\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu()\r\n",
        "    real_values = torch.stack(real_values).cpu()\r\n",
        "    return tweets, predictions, prediction_probs, real_values"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# This section is used to calculate predictions and save them to a .csv file\r\n",
        "# or load them from the saved .csv file from the pretrained model.\r\n",
        "if not PRETRAINED:\r\n",
        "    y_tweets, y_pred, y_pred_probs, y_test = get_predictions(\r\n",
        "        model, test_data_loader)\r\n",
        "    y_pred_probs = torch.squeeze(y_pred_probs.reshape(-1, 1))\r\n",
        "    y_pred_probs_bearish = y_pred_probs[:y_pred_probs.size(0)//2]\r\n",
        "    y_pred_probs_bullish = y_pred_probs[y_pred_probs.size(0)//2:]\r\n",
        "    outcome_df = pd.DataFrame({\"y_tweets\": y_tweets})\r\n",
        "    outcome_df['y_pred'] = y_pred\r\n",
        "    outcome_df['y_pred_probs_bearish'] = y_pred_probs_bearish\r\n",
        "    outcome_df['y_pred_probs_bullish'] = y_pred_probs_bullish\r\n",
        "    outcome_df['y_test'] = y_test\r\n",
        "    outcome_df.to_csv(f\"{path}twitter_sentiment_outcomes.csv\")\r\n",
        "else:\r\n",
        "    outcome_df = pd.read_csv(f\"{path}twitter_sentiment_outcomes.csv\")\r\n",
        "    y_tweets = outcome_df.y_tweets.tolist()\r\n",
        "    y_pred = torch.tensor(outcome_df.y_pred.tolist())\r\n",
        "    y_pred_probs_bearish = torch.tensor(\r\n",
        "        outcome_df.y_pred_probs_bearish.to_list()).reshape(-1, 2)\r\n",
        "    y_pred_probs_bullish = torch.tensor(\r\n",
        "        outcome_df.y_pred_probs_bullish.tolist()).reshape(-1, 2)\r\n",
        "    y_pred_probs = torch.cat((y_pred_probs_bearish, y_pred_probs_bullish), 0)\r\n",
        "    y_test = torch.tensor(outcome_df.y_test.tolist())\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "zHdPZr60-0c_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "a17758f5-5b8d-4e99-d24a-0d4c664804e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def show_confusion_matrix(confusion_matrix):\r\n",
        "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\r\n",
        "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\r\n",
        "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\r\n",
        "    plt.ylabel('True sentiment')\r\n",
        "    plt.xlabel('Predicted sentiment')\r\n",
        "\r\n",
        "\r\n",
        "cm = confusion_matrix(y_test, y_pred)\r\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\r\n",
        "show_confusion_matrix(df_cm)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tweepy import API\r\n",
        "from tweepy import Cursor\r\n",
        "from tweepy.streaming import StreamListener\r\n",
        "from tweepy import OAuthHandler\r\n",
        "from tweepy import Stream\r\n",
        "import re\r\n",
        "import time\r\n",
        "import api_credentials  # Update this file with your own Twitter and Alpaca credentials\r\n",
        "# This import only works on a local computer.\r\n",
        "# I did not write any code that allows this import to work when using Colab.\r\n",
        "\r\n",
        "class TwitterAuthenticator():\r\n",
        "    '''\r\n",
        "    Used to authenticate my Twitter Developer account for permission to use the Twitter API\r\n",
        "    '''\r\n",
        "    def authenticate_twitter_app(self):\r\n",
        "        TWITTER_ACCESS_TOKEN = api_credentials.TWITTER_ACCESS_TOKEN\r\n",
        "        TWITTER_ACCESS_TOKEN_SECRET = api_credentials.TWITTER_ACCESS_TOKEN_SECRET\r\n",
        "        TWITTER_CONSUMER_KEY = api_credentials.TWITTER_CONSUMER_KEY\r\n",
        "        TWITTER_CONSUMER_SECRET = api_credentials.TWITTER_CONSUMER_SECRET\r\n",
        "        auth = OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)\r\n",
        "        auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\r\n",
        "        return auth\r\n",
        "\r\n",
        "\r\n",
        "class TwitterClient():\r\n",
        "    '''\r\n",
        "    A class for connecting to the Twitter API\r\n",
        "    '''\r\n",
        "    def __init__(self, twitter_user=None):\r\n",
        "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\r\n",
        "        self.twitter_client = API(self.auth)\r\n",
        "        self.twitter_user = twitter_user\r\n",
        "\r\n",
        "    def get_twitter_client_api(self):\r\n",
        "        return self.twitter_client\r\n",
        "\r\n",
        "    def get_user_timeline_tweets(self, num_tweets):\r\n",
        "        tweets = []\r\n",
        "        for tweet in Cursor(self.twitter_client.user_timeline, id=self.twitter_user).items(num_tweets):\r\n",
        "            tweets.append(tweet)\r\n",
        "        return tweets\r\n",
        "\r\n",
        "\r\n",
        "class TwitterStreamer():\r\n",
        "    \"\"\"\r\n",
        "    A class for streaming live tweets.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\r\n",
        "\r\n",
        "    def stream_tweets(self, hash_tag_list):\r\n",
        "        listener = TwitterListener()\r\n",
        "        stream = Stream(self.auth, listener)\r\n",
        "\r\n",
        "        # Filter Twitter streams by keywords\r\n",
        "        stream.filter(track=hash_tag_list)\r\n",
        "\r\n",
        "\r\n",
        "class TwitterListener(StreamListener):\r\n",
        "    '''\r\n",
        "    An exponentially increasing time-out penalty is applied to a developer's account each time Twitter's request limit is exceeded.\r\n",
        "    This listener will stop the program from running if it recieves an error code 420, \r\n",
        "    which is an indication that our program is about to exceed the tweet request limit.    \r\n",
        "    \r\n",
        "    A maximum of 450 recent search requests for tweets is allowed every 15 minutes.\r\n",
        "    '''\r\n",
        "    def on_error(self, status):\r\n",
        "        if status == 420:\r\n",
        "            # Return False on_data method in case rate limit occurs.\r\n",
        "            return False\r\n",
        "        print(status)\r\n",
        "\r\n",
        "\r\n",
        "class TweetPreprocessor():\r\n",
        "\r\n",
        "    def clean_tweet(self, tweet):\r\n",
        "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\r\n",
        "\r\n",
        "    def tweets_to_data_frame(self, tweets):\r\n",
        "        '''\r\n",
        "        Returns the data frame of tweets and the latest tweet id, \r\n",
        "        so that the next function call will only include tweets after \r\n",
        "        the latest tweet id to avoid duplication.\r\n",
        "\r\n",
        "        Input: tweepy.models.SearchResults (a JSON data structure of tweets)\r\n",
        "        Outputs: a pandas data frame of recent tweets and the latest tweet id\r\n",
        "        '''\r\n",
        "        df = pd.DataFrame(data=[self.clean_tweet(tweet.text) for tweet in tweets], columns=['tweet'])\r\n",
        "\r\n",
        "        df['id'] = np.array([tweet.id for tweet in tweets])\r\n",
        "\r\n",
        "        # The lastest tweet id is conveiniently the largest number\r\n",
        "        latest_id = df.id.max()\r\n",
        "\r\n",
        "        # # If you want to drop retweets from data frame\r\n",
        "        # df = df[~df.tweet.str.contains(\"RT\")]\r\n",
        "\r\n",
        "        return df, latest_id\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def predict_live_tweets(model, df, risk_level=0.5):\r\n",
        "    '''\r\n",
        "    The method that handles prediction of tweet sentiment accepts as arguments the sentiment \r\n",
        "    classification model, the Pandas data frame of recent tweets, and the user’s desired risk \r\n",
        "    level for stock trading, which is a value between zero (most conservative) and one (most risky). \r\n",
        "    \r\n",
        "    Next, it determines whether to trade a stock based on the certainty of the model’s prediction \r\n",
        "    about the sentiment of tweets it has just analyzed. If the user sets their desired risk level \r\n",
        "    closer to zero, then the BERT model is required to be more certain about its prediction before \r\n",
        "    making a trade, thus decreasing the volume of trades that occur. Since the BERT model computes \r\n",
        "    a likelihood score for both the bullish and bearish classifications for a tweet, certainty is \r\n",
        "    defined as the difference between the two scores. \r\n",
        "\r\n",
        "    The method also keeps a tally for a total polarity score for the incoming data frame, which \r\n",
        "    increments by one with each certain bullish prediction and decrements by one with each certain \r\n",
        "    bearish prediction. Bullish and bearish predictions that do not meet the minimum level of \r\n",
        "    certainty specified by the risk level do not affect the polarity score.\r\n",
        "\r\n",
        "    The absolute value of the polarity score functions as a multiplier that contributes towards \r\n",
        "    the decision of how many shares to buy of the specified stock. Thus, a polarity score of zero \r\n",
        "    multiplies by zero, which results in not making any trades with that stock. Essentially, a \r\n",
        "    score of zero means that the accumulated sentimental predictions of certainty from all of \r\n",
        "    the tweets in the data frame was neutral, and this begins to happen more often as the user \r\n",
        "    decreases the input of the risk level of their trading strategy.\r\n",
        "    '''\r\n",
        "    trade_bool = False\r\n",
        "    polarity = 0\r\n",
        "    min_certainty = 7 - (risk_level*4)\r\n",
        "    predictions = []\r\n",
        "    for tweet in df.tweet:\r\n",
        "        encoded_tweet = tokenizer.encode_plus(tweet, max_length=MAX_LEN, add_special_tokens=True,\r\n",
        "            return_token_type_ids=False, padding=True, return_attention_mask=True, return_tensors='pt')\r\n",
        "        input_ids = encoded_tweet['input_ids'].to(device)\r\n",
        "        attention_mask = encoded_tweet['attention_mask'].to(device)\r\n",
        "        output = model(input_ids, attention_mask)\r\n",
        "        max_score, prediction = torch.max(output, dim=1)\r\n",
        "        predictions.append(prediction.item())\r\n",
        "        min_score, _ = torch.min(output, dim=1)\r\n",
        "        certainty = (max_score - min_score).item()\r\n",
        "        if certainty >= min_certainty:\r\n",
        "            if prediction.item() == 0:\r\n",
        "                polarity -= 1\r\n",
        "            else:\r\n",
        "                polarity += 1\r\n",
        "    if polarity > 0 or polarity < 0:\r\n",
        "        trade_bool = True\r\n",
        "\r\n",
        "    df['prediction'] = np.array(predictions)\r\n",
        "    \r\n",
        "    return polarity, trade_bool, df\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import alpaca_trade_api as tradeapi\r\n",
        "import api_credentials\r\n",
        "\r\n",
        "def cancel_pending_orders():\r\n",
        "    '''This section is only to be used to automate cancelling all pending\r\n",
        "orders that were made during code development outside of stock market hours'''\r\n",
        "    ALPACA_ENDPOINT_URL = api_credentials.ALPACA_ENDPOINT_URL\r\n",
        "    ALPACA_API_KEY = api_credentials.ALPACA_API_KEY\r\n",
        "    ALPACA_SECRET_KEY = api_credentials.ALPACA_SECRET_KEY\r\n",
        "    api = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY,\r\n",
        "                        ALPACA_ENDPOINT_URL, api_version='v2')\r\n",
        "\r\n",
        "    orders = api.list_orders()\r\n",
        "    if len(orders) > 0:\r\n",
        "        for order in orders:\r\n",
        "            api.cancel_order(order.id)\r\n",
        "\r\n",
        "# ### Do not uncomment, unless specifically canceling pending orders.\r\n",
        "# cancel_pending_orders()\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import alpaca_trade_api as tradeapi\r\n",
        "\r\n",
        "unique_minutes = []\r\n",
        "\r\n",
        "def alpaca_trader(ticker, polarity):\r\n",
        "    '''\r\n",
        "    Approximate price of the stock, then calculates a value equivalent to 1% of the current portfolio value divided \r\n",
        "    by the current price of the stock, which is then multiplied by the polarity score to determine the quantity of \r\n",
        "    shares to buy. If the buying power is exceeded by this amount, then the quantity is decremented by one share \r\n",
        "    until the expense is affordable for the account to make the purchase or the quantity of shares to buy is zero.\r\n",
        "\r\n",
        "    Before selling shares of a stock, the bot needs to determine if it even owns any of that stock to avoid throwing \r\n",
        "    an error by trying to sell something it does not own. If it does own that stock, it then decides to sell the \r\n",
        "    quantity of shares in the same way that it determines the number of shares to buy, using a combination of polarity \r\n",
        "    score, portfolio value, and current approximate price per share. If that quantity is greater than the number of \r\n",
        "    shares currently owned, then the bot simply sells all of that stock.\r\n",
        "\r\n",
        "    With all this math and fact checking, there is still room for error because the current stock price is always \r\n",
        "    an approximation since traders are buying and selling stock at various prices within milliseconds of each other. \r\n",
        "    Thus, the order is placed within a try-except block and marked as a valid trade once complete, inspired by how \r\n",
        "    mutual exclusion locks work with parallel programming systems. If an error is thrown because the trade expense \r\n",
        "    is suddenly too expensive within milliseconds, the bot decrements the quantity of shares to buy by one and tries \r\n",
        "    again. If the quantity decreases to zero before it becomes affordable, then the transaction is marked as “skipped”, \r\n",
        "    terminates the trading process, and exits the function. \r\n",
        "    '''\r\n",
        "    global unique_minutes\r\n",
        "    ALPACA_ENDPOINT_URL = api_credentials.ALPACA_ENDPOINT_URL\r\n",
        "    ALPACA_API_KEY = api_credentials.ALPACA_API_KEY\r\n",
        "    ALPACA_SECRET_KEY = api_credentials.ALPACA_SECRET_KEY\r\n",
        "    api = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, ALPACA_ENDPOINT_URL, api_version='v2')\r\n",
        "    account = api.get_account()\r\n",
        "    market_clock = api.get_clock()\r\n",
        "    minute = int(str(market_clock.timestamp)[14:16])\r\n",
        "    frequency = 10 # minutes\r\n",
        "    if minute < 2 and len(unique_minutes) == 6:\r\n",
        "        unique_minutes = []\r\n",
        "    with open('portfolio_performance.txt', 'a') as f:\r\n",
        "        # Write to file every {frequency} minutes\r\n",
        "        if minute % frequency == 0:\r\n",
        "            if minute not in unique_minutes:\r\n",
        "                unique_minutes.append(minute)\r\n",
        "                f.write(f\"Equity: {account.equity}, Time Stamp: {market_clock.timestamp} \\n\")\r\n",
        "\r\n",
        "\r\n",
        "    barset = api.get_barset(ticker[1:], 'day', limit=1)\r\n",
        "    open_price = barset[ticker[1:]][0].o\r\n",
        "    close_price = barset[ticker[1:]][0].c\r\n",
        "    approximate_price_per_share = (open_price + close_price)/2\r\n",
        "    # Determine how many shares to buy based on the price of the stock.\r\n",
        "    # Currently allowing for 1% of portfolio per trade.\r\n",
        "    shares_per_polarity_point = (float(account.portfolio_value) * 0.01) // approximate_price_per_share\r\n",
        "\r\n",
        "    with open('stock_trading_decisions.txt', 'a') as f:\r\n",
        "        msg = f\"Time Stamp: {market_clock.timestamp} \\n\"\r\n",
        "        print(msg)\r\n",
        "        f.write(msg)\r\n",
        "        \r\n",
        "        if market_clock.is_open:\r\n",
        "            if polarity > 0:\r\n",
        "                side = \"buy\"\r\n",
        "                qty = polarity*shares_per_polarity_point\r\n",
        "                expense = approximate_price_per_share * qty\r\n",
        "\r\n",
        "                # If buying power is limited, then decrease quantity of shares until transaction amount is lower than buying power\r\n",
        "                while expense > float(account.buying_power):\r\n",
        "                    qty -= 1\r\n",
        "                    expense = approximate_price_per_share * qty\r\n",
        "            else:\r\n",
        "                side = \"sell\"\r\n",
        "                polarity *= -1\r\n",
        "                qty = polarity*shares_per_polarity_point\r\n",
        "\r\n",
        "                # Check how many shares I currently own, if any\r\n",
        "                # try except because an error is thrown if zero shares are owned.\r\n",
        "                try:\r\n",
        "                    pos_qty = float(api.get_position(ticker[1:]).qty)\r\n",
        "                except Exception as exception:\r\n",
        "                    if exception.__str__() == 'position does not exist':\r\n",
        "                        pos_qty = 0\r\n",
        "                if qty > pos_qty:\r\n",
        "                    qty = pos_qty\r\n",
        "\r\n",
        "            # only perform a trade if trading more than 0 shares\r\n",
        "            if qty > 0:\r\n",
        "                # Sometimes the prices change and throw a buying power error. Decrease qty until satisfied.\r\n",
        "                invalid = True\r\n",
        "                skipped = False\r\n",
        "                while invalid:\r\n",
        "                    try:\r\n",
        "                        if qty == 0:\r\n",
        "                            skipped = True\r\n",
        "                            break\r\n",
        "                        # market: buy or sell at market price, opposed to a limit order.\r\n",
        "                        # time_in_force: only keep order open until end of the day\r\n",
        "                        order = api.submit_order(\r\n",
        "                            symbol=ticker[1:], qty=qty, side=side, type=\"market\", time_in_force=\"day\")\r\n",
        "                        invalid = False\r\n",
        "                    except Exception as exception:\r\n",
        "                        if exception.__str__() == 'insufficient buying power':\r\n",
        "                            qty -= 1\r\n",
        "                if not skipped:\r\n",
        "                    if order.status == 'accepted':\r\n",
        "                        msg = f\"Success! Order placed to {order.side} {order.qty} shares of {ticker}. \\n\"\r\n",
        "                        print(msg)\r\n",
        "                        f.write(msg)\r\n",
        "                    else:\r\n",
        "                        msg = f\"Trade failed. Alpaca account status: {account.status}. \\n\"\r\n",
        "                        print(msg)\r\n",
        "                        f.write(msg)\r\n",
        "                else:\r\n",
        "                    msg = f\"Transaction prices changed during processing. Either not enough buying power or insufficient shares to sell. Skipping. \\n\"\r\n",
        "                    print(msg)\r\n",
        "                    f.write(msg)\r\n",
        "                time.sleep(3)\r\n",
        "            else:\r\n",
        "                if side == \"buy\":\r\n",
        "                    msg = f\"You don't have enough buying power to buy {ticker}. Skipping. \\n\"\r\n",
        "                    print(msg)\r\n",
        "                else:\r\n",
        "                    msg = f\"You do not own any shares of {ticker} to sell. Skipping. \\n\"\r\n",
        "                    print(msg)\r\n",
        "            time.sleep(3)\r\n",
        "        else:\r\n",
        "            msg = f\"No orders were made because the stock market is currently closed for trading. \\n\"\r\n",
        "            print(msg)\r\n",
        "        time.sleep(3)\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def main():\r\n",
        "    '''\r\n",
        "    https://developer.twitter.com/en/docs/twitter-api/rate-limits\r\n",
        "\r\n",
        "    I can make a maximum of 450 recent search requests for tweets every 15 minutes.\r\n",
        "    Making more requests than this will result in an error.\r\n",
        "    This works out to one request made every 2 seconds.\r\n",
        "\r\n",
        "    To be safe, since time.sleep is approximate - I will pause for 5 seconds after tweet queries\r\n",
        "    and 3 seconds after stock order queries, which is more than enough wait time.\r\n",
        "\r\n",
        "    This means that a particular ticker, such as $AAPL, will be checked on every time.sleep(#) * len(hash_tag_list) seconds.\r\n",
        "    '''\r\n",
        "    twitter_client = TwitterClient()\r\n",
        "    tweet_preprocessor = TweetPreprocessor()\r\n",
        "    api = twitter_client.get_twitter_client_api()\r\n",
        "    latest_id = None\r\n",
        "\r\n",
        "    # This is the portfolio\r\n",
        "    hash_tag_dict = {\"$AAPL\": None, \"$ABNB\": None, \"$AMD\": None, \"$AMZN\": None, \"$BA\": None, \"$DIS\": None,\r\n",
        "                     \"$FB\": None, \"$GOOGL\": None, \"$NFLX\": None, \"$NVDA\": None, \"$SPY\": None, \"$TSLA\": None, \"$VZ\": None, \"$XOM\": None}\r\n",
        "    while True:\r\n",
        "        for ticker in hash_tag_dict.keys():\r\n",
        "            print(f\"Searching for recent tweets about {ticker}\")\r\n",
        "            tweets = api.search(q=ticker, lang='en', result_type='recent', since_id=hash_tag_dict[ticker])\r\n",
        "            df, latest_id = tweet_preprocessor.tweets_to_data_frame(tweets)\r\n",
        "            # update the value of the current ticker to the latest tweet id\r\n",
        "            hash_tag_dict[ticker] = latest_id\r\n",
        "            polarity, trade_bool, df = predict_live_tweets(model, df, risk_level=RISK_LEVEL)\r\n",
        "\r\n",
        "            # ### useful print codes to illustrate what is happening:\r\n",
        "            # print(df[['tweet', 'prediction']])  \r\n",
        "            # print(df[['tweet', 'id']])\r\n",
        "            print()\r\n",
        "\r\n",
        "            if trade_bool:\r\n",
        "                if polarity > 0:\r\n",
        "                    print(f\"Positive sentiment polarity score of {polarity}. Preparing to buy {ticker} stock.\")\r\n",
        "                else:\r\n",
        "                    print(f\"Negative sentiment polarity score of {polarity}. Preparing to sell {ticker} stock.\")\r\n",
        "                print()\r\n",
        "                alpaca_trader(ticker, polarity)\r\n",
        "            else:\r\n",
        "                print(f\"Neutral sentiment polarity score. Hold off on trading {ticker} stock.\")\r\n",
        "            time.sleep(5)  # seconds\r\n",
        "            print()\r\n",
        "            print('--------------------')\r\n",
        "            print()\r\n",
        "main()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Keep this cell here to fix the print window from the cell above in one place.\r\n",
        "# Or delete this cell to let the print statement forever increase downwards."
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}
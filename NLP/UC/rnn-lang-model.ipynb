{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# An RNN Transducer-based Language ModelÂ¶\n",
    "\n",
    "In this problem we will\n",
    "- Build an LSTM transducer-based language model using early stopping and compute the text perplexity.\n",
    "- Use the model to generate sentences.\n",
    "- Extend the model and compare performance when we \n",
    "    - replace the LSTM with a GRU or a Simple RNN\n",
    "    - increase the number of LSTM layers\n",
    "    - add dropout\n",
    "    - add gradient clipping\n",
    "    \n",
    "You can develop on your local machine, but to train on the full training set requires GPUs.  We recommend using the GPUs at [Google Colab](https://colab.research.google.com). To upload a notebook, choose the \"Files\" dropdown menu and then \"Upload.\"  To use a GPU, choose Runtime > Change runtime type and select GPU.    \n",
    "    \n",
    "Acknowledgement:  This assignment was originally written by Zewei Chu, and was inspired by a [homework in CS287](https://github.com/harvard-ml-courses/cs287-s18/blob/master/HW2/Homework%202.ipynb) at Harvard.\n",
    "    "
   ],
   "metadata": {
    "colab_type": "text",
    "id": "FiIVgmEqBBkh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Development vs full version\n",
    "\n",
    "Choose the appropriate version using the switches `DEVELOPING` and `COLAB.`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "source": [
    "!pip install torchtext==0.9.0\r\n",
    "!pip install torch==1.8.1\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchtext==0.9.0 in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: torch==1.8.0 in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.9.0) (1.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.9.0) (4.60.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.9.0) (2.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchtext==0.9.0) (1.21.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.9.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.9.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.9.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchtext==0.9.0) (2020.12.5)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hocke\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "source": [
    "import torchtext\r\n",
    "from torchtext.vocab import Vectors\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "\r\n",
    "USE_CUDA = torch.cuda.is_available()\r\n",
    "\r\n",
    "if USE_CUDA:\r\n",
    "    DEVICE = torch.device('cuda')\r\n",
    "    print(\"Using cuda.\")\r\n",
    "else:\r\n",
    "    DEVICE = torch.device('cpu')\r\n",
    "    print(\"Using cpu.\")\r\n",
    "\r\n",
    "seed = 53113    \r\n",
    "random.seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "torch.manual_seed(seed)\r\n",
    "if USE_CUDA:\r\n",
    "    torch.cuda.manual_seed(seed)\r\n",
    "\r\n",
    "# Change the following to false when training on\r\n",
    "# the full set\r\n",
    "# DEVELOPING = True    \r\n",
    "DEVELOPING = False\r\n",
    "\r\n",
    "if DEVELOPING:\r\n",
    "    print('Small development version')\r\n",
    "    BATCH_SIZE = 4\r\n",
    "    EMBEDDING_SIZE = 20\r\n",
    "    MAX_VOCAB_SIZE = 5000\r\n",
    "    TRAIN_DATA_SET = \"lm-train-small.txt\"\r\n",
    "    DEV_DATA_SET = \"lm-dev-small.txt\"\r\n",
    "    TEST_DATA_SET = \"lm-test-small.txt\"\r\n",
    "    BPTT_LENGTH = 8\r\n",
    "    COLAB = False\r\n",
    "    #COLAB = True\r\n",
    "else:\r\n",
    "    print('Full version')\r\n",
    "    BATCH_SIZE = 32\r\n",
    "    EMBEDDING_SIZE = 650\r\n",
    "    MAX_VOCAB_SIZE = 50000\r\n",
    "    TRAIN_DATA_SET = \"lm-train.txt\"\r\n",
    "    DEV_DATA_SET = \"lm-dev.txt\"\r\n",
    "    TEST_DATA_SET = \"lm-test.txt\"\r\n",
    "    BPTT_LENGTH = 32\r\n",
    "    # COLAB = False\r\n",
    "    COLAB = True\r\n",
    "\r\n",
    "# For uploading data to Colab see, e.g., \r\n",
    "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \r\n",
    "\r\n",
    "if COLAB:\r\n",
    "    from google.colab import drive\r\n",
    "    drive.mount('/content/drive', force_remount=True)\r\n",
    "    PATH = \"/content/drive/MyDrive/datasets\"\r\n",
    "else:\r\n",
    "    PATH = \".\\datasets\"\r\n",
    "    \r\n",
    "LOG_FILE = \"language-model.log\"\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu.\n",
      "Full version\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "kvGYWOeMBBkj",
    "outputId": "e48f60e9-d504-493c-8910-afd8a9fccb61"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing using the legacy component of TorchText\n",
    "\n",
    "TorchText is being upgraded.  For our preprocessing we have use its legacy component.  [Documentation](https://torchtext.readthedocs.io/en/latest/index.html) for this legacy component torchtext is relatively sparse (and, unfortunately, not very clear), but [Ben Trevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb) has a useful tutorial.  (If you are keen to understand this component, you may also want to look at the [source code](https://github.com/pytorch/text/tree/master/torchtext/legacy).)\n",
    "\n",
    "All the **legacy torchtext code is already provided**. \n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "FQIs9OK2BBkj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "source": [
    "TEXT = torchtext.legacy.data.Field(lower=True)\r\n",
    "\r\n",
    "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=PATH, \r\n",
    "    train=TRAIN_DATA_SET, validation=DEV_DATA_SET, test=TEST_DATA_SET, text_field=TEXT)\r\n",
    "\r\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\r\n",
    "VOCAB_SIZE = len(TEXT.vocab)\r\n",
    "\r\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\r\n",
    "\r\n",
    "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\r\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=DEVICE, bptt_len=BPTT_LENGTH, \r\n",
    "    repeat=False)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 50002\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fe2FRmolBBkn",
    "outputId": "26cf07ed-61a6-4e45-e126-c2cdf5cef183"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Back propagation through time (BPTT) iterator\n",
    "\n",
    "The [BPTTIterator](https://torchtext.readthedocs.io/en/latest/data.html#bpttiterator) is a custom torchtext iterator for language modeling using RNNs.  Suppose the text in an example is \"the quick brown fox\".  The target in the transducer-based RNN language model would then be \"quick brown fox jumps\".  This allows every prefix of the text to be used as an training example, with the corresponding word in the target text as the target word.  So the above would lead to four examples, written as text sequence -> target word:\n",
    "* \"the\" -> \"quick\"\n",
    "* \"the quick\" -> \"brown\"\n",
    "* \"the quick brown\" -> \"fox\"\n",
    "* \"the quick brown fox\" -> \"jump\"\n",
    "\n",
    "(Unlike some of the examples in class, here we treat words as part of a sequence without special consideration for sentences.  In particular, we don't use start/end of setence tags.)\n",
    "\n",
    "One very **significant feature** of the BPTTIterator is that examples continue across batches.  To illustrate let the original data be one long seqence $w_1, w_2, \\ldots, w_N$, in which, say, $N = 4,000$.  Further let each batch consist of $4$ examples, each of length 8.  Then the first batch created by BPTTIterator would be the following 4 examples---\n",
    "\n",
    "- $(w_1, w_2, \\ldots, w_{8}), (w_{1001}, w_{1002}, \\ldots, w_{1008}), \\ldots, (w_{3001}, w_{3002}, \\ldots, w_{3008}).$ \n",
    "\n",
    "and the second batch would be---\n",
    "\n",
    "- $(w_{9}, w_{10}, \\ldots, w_{16}), (w_{1009}, w_{1010}, \\ldots, w_{1016}), \\ldots, (w_{3009}, w_{3010}, \\ldots, w_{3016}).$\n",
    "\n",
    "This has implications on how the hidden state of the RNN is set for the second batch onwards."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "3y7Us1voBBkr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "source": [
    "it = iter(train_iter)\r\n",
    "batch = next(it)\r\n",
    "print(\"The first three text/target sequences from the first batch are:\\n\")\r\n",
    "indent = \" \" * 4\r\n",
    "for j in range(3):\r\n",
    "    print(indent, f\"Text Sequence {j}:\", \r\n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\r\n",
    "    print(indent, f\"Target Sequence {j}:\",\r\n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\r\n",
    "    print()\r\n",
    " \r\n",
    "print(f\"Each sequence has BPTT_LENGTH = {BPTT_LENGTH}.\\n\")\r\n",
    "print(\"Also the sequences continue in the next batch!\\n\")\r\n",
    "batch = next(it)\r\n",
    "for j in range(3):\r\n",
    "    print(indent, f\"Text Sequence {j}:\", \r\n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\r\n",
    "    print(indent, f\"Target Sequence {j}:\",\r\n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\r\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The first three text/target sequences from the first batch are:\n",
      "\n",
      "     Text Sequence 0: anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term\n",
      "     Target Sequence 0: originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is\n",
      "\n",
      "     Text Sequence 1: had dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms\n",
      "     Target Sequence 1: dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms took\n",
      "\n",
      "     Text Sequence 2: in eastern asia after the great expansion from the african continent and a second expansion from the area of northern siberia which would make it consistent with the early <unk> period history\n",
      "     Target Sequence 2: eastern asia after the great expansion from the african continent and a second expansion from the area of northern siberia which would make it consistent with the early <unk> period history at\n",
      "\n",
      "Each sequence has BPTT_LENGTH = 32.\n",
      "\n",
      "Also the sequences continue in the next batch!\n",
      "\n",
      "     Text Sequence 0: is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by\n",
      "     Target Sequence 0: still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self\n",
      "\n",
      "     Text Sequence 1: took form once atoms become neutral they only absorb photons of a discrete absorption spectrum this allows most of the photons in the universe to travel unimpeded for billions of years these\n",
      "     Target Sequence 1: form once atoms become neutral they only absorb photons of a discrete absorption spectrum this allows most of the photons in the universe to travel unimpeded for billions of years these photons\n",
      "\n",
      "     Text Sequence 2: at first contact with the japanese people was friendly and both were equals in a trade relationship however eventually the japanese started to dominate the relationship and soon established large settlements on\n",
      "     Target Sequence 2: first contact with the japanese people was friendly and both were equals in a trade relationship however eventually the japanese started to dominate the relationship and soon established large settlements on the\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "fqBOF32nBBks",
    "outputId": "9815b12a-f9c9-49d2-b04b-baa46c51dcad"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initializing hidden vectors from the detached hidden vectors of previous batch\n",
    "\n",
    "Since sequences continue across batches, for proper training, **the final output hidden vectors in a batch should be used to initialize the hidden vectors for the next batch**.  But care should be taken to detach vectors used for initialization from the computational graph, else gradients would flow \"from one batch to the previous\" and training would be increasingly slow. "
   ],
   "metadata": {
    "colab_type": "text",
    "id": "smzi0zXzBBkv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the model"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "3B853z05BBkv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Our RNN based language model (when using an LSTM) for a language model is as follows:\n",
    "- Let the input sequence---the *context*---be $w_1, w_2, \\ldots, w_n$, and let the target sequence be $w_2, \\ldots, w_n, w_{n+1}$.\n",
    "- At step $i$ of the input, for $1 \\leq i \\leq n$:\n",
    "    - $x_i = E_{[w_i]}$.\n",
    "    - $y_i, (h_i, c_i) = \\text{LSTM}(x_i, (h_{i-1}, c_{i-1}))$.  For LSTMs, $y_i$ equals $h_i$.\n",
    "    - $\\widehat{y}_i = \\text{softmax}(y_i W + b)$, in which $\\widehat{y}_i$ is the predicted probability distribution for $w_{i+1}$.\n",
    "    - In the above \n",
    "        - $x_i$ is $1 \\times \\text{embedding dim}$ \n",
    "        - $y_i$, $h_i$ and $c_i$ are $1 \\times \\text{hidden dim}$\n",
    "        - $\\widehat{y}_i$ is $1 \\times \\text{vocab size}$.\n",
    "- The loss $\\ell = \\sum_{i=1}^n \\log \\widehat{y}_{i_{[w_{i+1}]}}$, in which $\\log \\widehat{y}_{i_{[w_{i+1}]}}$ is the component of $\\log \\widehat{y}_{i}$ corresponding to the element $w_{i+1}$.\n",
    "\n",
    "Since the sequences continue across batches we retain the hidden states across batches. Specifically, consider the $k$th example in batch $j$.  For $j=1$, i.e., first batch, the corresponding $(h_0, c_0)$ for the $k$th example is set to all zeros.  But for $j > 1$, the corresponding $(h_0, c_0)$ is set to $(h_{n}, c_{n})$ of the $k$th example in batch $j-1$.\n",
    "\n",
    "In PyTorch we do not call the forward function separately for each step $i$.  Instead we call the model with\n",
    "\n",
    "- tensors corresponding to $(w_1, w_2, \\ldots, w_n)$ and $(h_0, c_0)$\n",
    "\n",
    "and receive as ouput\n",
    "\n",
    "- $(y_1, y_2, \\ldots, y_n)$ and $(h_n, c_n)$.\n",
    "\n",
    "Further the above is combined for several examples into one batch.  Please read the PyTorch documentation to learn more about building models\n",
    "            with RNNs.  E.g., see the documentation on [LSTMs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) and [Robert Gutherie's Tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py) on working with LSTMs.\n",
    "            \n",
    "The above can be adapted easily to [GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) or [Simple RNNs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) since the PyTorch interface is very similar.\n",
    "\n",
    "**Task 1** [10 points]: Complete the code for the class `RNNLM` based on the description above.  (Some extra parameters are provided since in a later task, you'll modify your code to incorporate the following: (i) replace the LSTM with a GRU or a Simple RNN, (ii) increase the number of LSTM layers, and (iii) add dropout.) "
   ],
   "metadata": {
    "colab_type": "text",
    "id": "_uge8v-wXj4Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "\r\n",
    "class RNNLM(nn.Module):\r\n",
    "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.5):\r\n",
    "        ''' Initialize model parameters corresponding to ---\r\n",
    "            - embedding layer\r\n",
    "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \r\n",
    "              optionally more than one layer\r\n",
    "            - linear layer to map from hidden vector to the vocabulary\r\n",
    "            - optionally, dropout layers.  Dropout layers can be placed after \r\n",
    "              the embedding layer or/and after the RNN layer. Dropout within\r\n",
    "              an RNN is only applied when there are two or more num_layers.\r\n",
    "            - optionally, initialize the model parameters.\r\n",
    "            \r\n",
    "            The arguments are:\r\n",
    "            \r\n",
    "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\r\n",
    "            vocab_size: size of vocabulary\r\n",
    "            embedding_dim: size of an embedding vector\r\n",
    "            hidden_dim: size of hidden/state vector in RNN\r\n",
    "            num_layers: number of layers in RNN\r\n",
    "            dropout: dropout probability.\r\n",
    "            \r\n",
    "        '''\r\n",
    "        super(RNNLM, self).__init__()\r\n",
    "        ## YOUR CODE HERE ##\r\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\r\n",
    "        self.rnn_type = rnn_type\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.hidden_dim = hidden_dim\r\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\r\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\r\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\r\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size) # fully connected hidden layer\r\n",
    "        \r\n",
    "    def forward(self, input, hidden0):\r\n",
    "        ''' \r\n",
    "        Run forward propagation for a given minibatch of inputs using\r\n",
    "        hidden0 as the initial hidden state.\r\n",
    "        In LSTMs hidden0 = (h0, c0). \r\n",
    "        The output of the RNN includes the hidden vector hiddenn = (hn, cn).\r\n",
    "        Return this as well so that it can be used to initialize the next\r\n",
    "        batch.\r\n",
    "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\r\n",
    "        the more efficient CrossEntropyLoss.  See \r\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\r\n",
    "        '''\r\n",
    "        ###YOUR CODE HERE###\r\n",
    "        x = self.embeddings(input)  # 1 x embedding_dim\r\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(DEVICE) \r\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(DEVICE)  \r\n",
    "        hidden0 = (h0, c0)\r\n",
    "\r\n",
    "        if self.rnn_type == \"RNN\":\r\n",
    "          yi, _ = self.rnn(x, h0)  # 1 x hidden_dim\r\n",
    "        elif self.rnn_type == \"GRU\":\r\n",
    "          yi, _ = self.gru(x, h0)  # 1 x hidden_dim\r\n",
    "        elif self.rnn_type == \"LSTM\":\r\n",
    "          yi, _ = self.lstm(x, hidden0)  # 1 x hidden_dim\r\n",
    "        else:\r\n",
    "          print(f\"{self.rnn_type} is not a valid rnn_type. Try RNN, LSTM, or GRU.\")\r\n",
    "          return\r\n",
    "\r\n",
    "        yi_hat = self.linear(yi) \r\n",
    "        return yi_hat, hidden0\r\n",
    "        \r\n",
    "    def init_state(self, sequence_length):\r\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.hidden_dim),\r\n",
    "                torch.zeros(self.num_layers, sequence_length, self.hidden_dim))\r\n",
    " \r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beflzeEkBBkw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate on a given data set\n",
    "\n",
    "The function for evaluation is provided below."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "EOu67_BOBBky"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "source": [
    "def evaluate(model, data):\r\n",
    "    '''\r\n",
    "    Evaluate the model on the given data.\r\n",
    "    '''\r\n",
    "    model.eval()\r\n",
    "    it = iter(data)\r\n",
    "    total_count = 0. # Number of target words seen\r\n",
    "    total_loss = 0. # Loss over all target words\r\n",
    "    with torch.no_grad():\r\n",
    "        # No gradients need to be maintained during evaluation\r\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\r\n",
    "        hidden = None \r\n",
    "        for i, batch in enumerate(it):\r\n",
    "            ''' \r\n",
    "              Do the following:\r\n",
    "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \r\n",
    "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\r\n",
    "                  https://pytorch.org/docs/stable/notes/cuda.html.\r\n",
    "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\r\n",
    "                  the current batch. \r\n",
    "                - Call forward propagation to get output and final hidden state vector.\r\n",
    "                - Compute the cross entropy loss\r\n",
    "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\r\n",
    "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \r\n",
    "                  total count (of target words) and total loss see so far over all batches.\r\n",
    "            '''\r\n",
    "            text, target = batch.text, batch.target\r\n",
    "            if USE_CUDA:\r\n",
    "                text, target = text.cuda(), target.cuda()\r\n",
    "            output, hidden = model(text, hidden)\r\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\r\n",
    "                  \r\n",
    "            total_count += np.multiply(*text.size())\r\n",
    "            total_loss += loss.item()*np.multiply(*text.size())\r\n",
    "                \r\n",
    "    loss = total_loss / total_count\r\n",
    "    model.train()\r\n",
    "    return loss\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po5NsbTOBBkz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model\n",
    "\n",
    "Training the model is mostly similar to previous homework sets except for:\n",
    "- A detached hidden vector is applied to the second batch onwards as described above.\n",
    "- Every, say, 10,000 iterations evaluate the model on a validation set, and if the mean loss is the lowest so far, save a copy of it.  After training, this \"best model\" is used for testing. \n",
    "      \n",
    "**Task 2** [15]: Complete the code below for training the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "source": [
    "GRAD_CLIP = 1 # [0, 0.5, 1]\r\n",
    "NUM_EPOCHS = 2\r\n",
    "NUM_LAYERS = 2  # [2, 3, 4, 5, 6]\r\n",
    "DROPOUT = 0.5 # [0, 0.25, 0.5, 0.75, 1]\r\n",
    "\r\n",
    "def repackage_hidden(h):\r\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\r\n",
    "    if h is None:\r\n",
    "        return None\r\n",
    "    elif isinstance(h, torch.Tensor):\r\n",
    "        return h.detach()\r\n",
    "    else:\r\n",
    "        return tuple(repackage_hidden(v) for v in h)\r\n",
    "\r\n",
    "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, dropout=DROPOUT)\r\n",
    "if USE_CUDA:\r\n",
    "    model = model.cuda()\r\n",
    "\r\n",
    "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\r\n",
    "learning_rate = 0.001\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "val_losses = []\r\n",
    "best_model = None\r\n",
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    model.train()\r\n",
    "    it = iter(train_iter)\r\n",
    "    # There are no hidden tensors for the first batch, and so will default to zeros.\r\n",
    "    hidden = None\r\n",
    "    for i, batch in enumerate(it):\r\n",
    "      ''' Do the following:\r\n",
    "          - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \r\n",
    "            the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\r\n",
    "            https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda\r\n",
    "          - Pass the hidden state vector from output of previous batch as the initial hidden vector for\r\n",
    "            the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\r\n",
    "            the provided repackage_hidden(). See\r\n",
    "            https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\r\n",
    "          - Zero out the model gradients to reset backpropagation for current batch\r\n",
    "          - Call forward propagation to get output and final hidden state vector.\r\n",
    "          - Compute the cross entropy loss\r\n",
    "          - Run back propagation to set the gradients for each model parameter.\r\n",
    "          - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\r\n",
    "            https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\r\n",
    "          - Run a step of gradient descent. \r\n",
    "          - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\r\n",
    "          - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\r\n",
    "            your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\r\n",
    "            copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \r\n",
    "            https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\r\n",
    "            in Sec 2.3.1 of Lecture notes by Cho: \r\n",
    "            https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\r\n",
    "      '''\r\n",
    "      ###YOUR CODE HERE###\r\n",
    "      text, target = batch.text, batch.target\r\n",
    "      if USE_CUDA:\r\n",
    "          text, target = text.cuda(), target.cuda()\r\n",
    "      hidden = repackage_hidden(hidden)\r\n",
    "      optimizer.zero_grad()\r\n",
    "      yi_hat, hidden = model(text, hidden)  # forward propogation\r\n",
    "      loss = loss_fn(yi_hat.view(-1, yi_hat.size(-1)), target.view(-1))\r\n",
    "      loss.backward() # backward propogation\r\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\r\n",
    "      optimizer.step()\r\n",
    "      if DEVELOPING:\r\n",
    "        if i % 100 == 0:\r\n",
    "          print(f'At iteration {i} the loss is {loss:.3f}.')\r\n",
    "      else:\r\n",
    "        if i % 1000 == 0:\r\n",
    "          print(f'At iteration {i} the loss is {loss:.3f}.')\r\n",
    "      min_loss = float('inf')\r\n",
    "      if i % 10000 == 0:\r\n",
    "        if loss < min_loss:\r\n",
    "          best_model = type(model)(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, dropout=DROPOUT)  # get a new instance\r\n",
    "          best_model.load_state_dict(model.state_dict())  # copy weights and stuff\r\n",
    "        val_losses.append(loss)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "At iteration 0 the loss is 10.823.\n",
      "At iteration 1000 the loss is 6.632.\n",
      "At iteration 2000 the loss is 6.586.\n",
      "At iteration 3000 the loss is 6.421.\n",
      "At iteration 4000 the loss is 5.763.\n",
      "At iteration 5000 the loss is 6.255.\n",
      "At iteration 6000 the loss is 6.155.\n",
      "At iteration 7000 the loss is 5.970.\n",
      "At iteration 8000 the loss is 6.239.\n",
      "At iteration 9000 the loss is 5.952.\n",
      "At iteration 10000 the loss is 6.033.\n",
      "At iteration 11000 the loss is 6.189.\n",
      "At iteration 12000 the loss is 6.161.\n",
      "At iteration 13000 the loss is 5.930.\n",
      "At iteration 14000 the loss is 5.795.\n",
      "At iteration 0 the loss is 5.973.\n",
      "At iteration 1000 the loss is 5.973.\n",
      "At iteration 2000 the loss is 6.044.\n",
      "At iteration 3000 the loss is 5.974.\n",
      "At iteration 4000 the loss is 5.424.\n",
      "At iteration 5000 the loss is 5.974.\n",
      "At iteration 6000 the loss is 5.863.\n",
      "At iteration 7000 the loss is 5.744.\n",
      "At iteration 8000 the loss is 5.894.\n",
      "At iteration 9000 the loss is 5.627.\n",
      "At iteration 10000 the loss is 5.786.\n",
      "At iteration 11000 the loss is 5.949.\n",
      "At iteration 12000 the loss is 5.740.\n",
      "At iteration 13000 the loss is 5.692.\n",
      "At iteration 14000 the loss is 5.589.\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "8dlLJ5FTBBk1",
    "outputId": "bca77ba6-bab8-4ddc-ba4a-85979ff33e10"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "source": [
    "'''\r\n",
    "Evaluate the loss of best_model on the validation set and compute its perplexity.\r\n",
    "'''\r\n",
    "val_loss = evaluate(best_model, val_iter)\r\n",
    "print(\"perplexity: \", np.exp(val_loss))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "perplexity:  377.5835641936008\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x3ooK74UBBk4",
    "outputId": "07a33872-bd64-4ac3-bdba-0a23d8988979"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the best model to evaluate the test dataset. \n",
    "\n",
    "We expect a test perplexity of less than 250 on the full model after a couple of epochs."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "m6klWxMwBBk7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "source": [
    "'''\r\n",
    "Evaluate the loss of best_model on the test set and compute its perplexity.\r\n",
    "'''\r\n",
    "test_loss = evaluate(best_model, test_iter)\r\n",
    "print(\"perplexity: \", np.exp(test_loss))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "perplexity:  442.47708682557\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oE7CK7XxBBk7",
    "outputId": "dd21b3b5-8909-4bd1-8f0c-e2f51a0a0f83",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the model to generate some sentences\n",
    "\n",
    "**Task 3** [10]: Write code to generate random sentences.  Section 9.5 in the Goldberg textbook describes how this can be done.  Since we don't have a start symbol, for the first word simply pick a random word from the vocabulary.\n",
    "\n",
    "You'll notice that the full sequences don't make much sense, but subsequences sound reasonably correct. "
   ],
   "metadata": {
    "colab_type": "text",
    "id": "FZ7-S1FuBBk9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "source": [
    "'''\r\n",
    "Use the model to generate 5 random sequences of length 50 each.\r\n",
    "\r\n",
    "Pick a word, uniformly at random.\r\n",
    "Form an N-gram out of the picked word and the last Nâ1 words.\r\n",
    "Look up the probability of that particular N-gram.\r\n",
    "Generate a uniform random number between 0 and 1. \r\n",
    "If that number is smaller than the probability of your N-gram, \"accept\" the new word. Otherwise, go back to the start.\r\n",
    "'''\r\n",
    "###YOUR CODE HERE###\r\n",
    "def random_sentence_generator(model, start, sequence_len=50):\r\n",
    "    model.eval()\r\n",
    "    words = start.split(' ')\r\n",
    "    h0, c0 = model.init_state(len(words)) \r\n",
    "\r\n",
    "    for i in range(sequence_len):\r\n",
    "        x = torch.tensor([[TEXT.vocab[w] for w in words[i:]]])\r\n",
    "        yi_hat, (h0, c0) = model(x, (h0, c0))\r\n",
    "\r\n",
    "        last_word_logits = yi_hat[0][-1]\r\n",
    "        p = nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\r\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\r\n",
    "        words.append(TEXT.vocab.itos[word_index])\r\n",
    "        \r\n",
    "    sentence = \"\"\r\n",
    "    for word in words[:-1]:\r\n",
    "        sentence += word + \" \"\r\n",
    "    sentence += f\"{words[-1]}.\"\r\n",
    "    return sentence\r\n",
    "\r\n",
    "\r\n",
    "for i in range(5):\r\n",
    "    start = TEXT.vocab.itos[np.random.randint(VOCAB_SIZE)]  # choose a random word from the vocabulary\r\n",
    "    print(random_sentence_generator(best_model, start))\r\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cory go to the academy of alexander s goal off his first down during his hands back soviet union alexander the term image was first one nine nine in armies who can be of judah the reporter in its name but as lens of winning lines of alexander s soldiers in.\n",
      "\n",
      "sternum drunken originates at both bengal wood and human beings western cities in asia based nor called d where the united nations complex and to solve it some types of the seven three during the delaware in several of the <unk> creators found null <unk> off freedom one seven irish invasion.\n",
      "\n",
      "stunned alexander new technology there is typically related networks example of computer science that had close to reducing the advance the euro approaches the outposts of the higher and turkish military coups in our super exports zur <unk> and newspapers such an approximately one terrorist players who until the orthodox empire.\n",
      "\n",
      "sponges not go passing line which were originally founded by the key when quantum but not listed from the armed forces to remain strong sir robert g two please a time during a complete the kingdom of party packed flowers co one two five it is called a mesh saying that.\n",
      "\n",
      "ubuntu the district to be more liberal and hearing of the general <unk> democratic identification of scrimmage there are called the habit toys against the interest in massachusetts stated this is not apply on sanskrit <unk> and other dissolved on sexual nature <unk> son of john alexander of three zero zero.\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LNSlzc-8BBk-",
    "outputId": "a6398d16-6119-4caf-83d6-1bc5bb2d4713"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose the best sentence from alternatives\n",
    "\n",
    "Generating random sentences as above is, however, not the objective of a language model.  Rather it is used as an auxiliary tool to choose the best sequence given some choices by comparing their perplexities.\n",
    "\n",
    "**Task 4** [5]: Use the code below to compute perplexities of the given six sentences.  Discuss the model's performance in choosing the best alternative.  (The code uses TorchText functions which are designed for much larger datasets.  So the perplexities below are approximate. Even so they illustrate the usefullness of our model.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "source": [
    "'''\r\n",
    "Notes on perplexity:\r\n",
    "Perplexity is the probability of the test set, normalized by the number of words.\r\n",
    "Minimizing perplexity is the same as maximizing probability. \r\n",
    "Perplexity is \"on average, how many different words could come next?\" \r\n",
    "If 10 words could come next each with 1/10 probability, then perplexity is 10. \r\n",
    "This is an average of the weighted probabilities. \"average branch factor\"\r\n",
    "\r\n",
    "Task 4:\r\n",
    "Developer Mode: \r\n",
    "Sentence 6 outperforms all of the other sentences with the lowest perplexity. \r\n",
    "This makes sense because the model is evaluating a set of words that we know make sense in sequence,\r\n",
    "and we then repeat those words.\r\n",
    "\r\n",
    "The model also impressively increase the perplexity after swapping the word \"world\" for \"cat\" as if to \r\n",
    "note that it makes more sense for us to be talking about world (plural implied) herd immunity to the virus, \r\n",
    "rather than a single cat.\r\n",
    "\r\n",
    "I do not have an explanation for why sen5 has a lower perplexity than sen1 - sen4. \r\n",
    "I suspect this is an error of running in Developer mode with a lack of training.\r\n",
    "\r\n",
    "Google Colab Mode:\r\n",
    "TBD - unable to run GPU.\r\n",
    "'''\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nNotes on perplexity:\\nPerplexity is the probability of the test set, normalized by the number of words.\\nMinimizing perplexity is the same as maximizing probability. \\nPerplexity is \"on average, how many different words could come next?\" \\nIf 10 words could come next each with 1/10 probability, then perplexity is 10. \\nThis is an average of the weighted probabilities. \"average branch factor\"\\n\\nTask 4:\\nDeveloper Mode: \\nSentence 6 outperforms all of the other sentences with the lowest perplexity. \\nThis makes sense because the model is evaluating a set of words that we know make sense in sequence,\\nand we then repeat those words.\\n\\nThe model also impressively increase the perplexity after swapping the word \"world\" for \"cat\" as if to \\nnote that it makes more sense for us to be talking about world (plural implied) herd immunity to the virus, \\nrather than a single cat.\\n\\nI do not have an explanation for why sen5 has a lower perplexity than sen1 - sen4. \\nI suspect this is an error of running in Developer mode with a lack of training.\\n\\nGoogle Colab Mode:\\nTBD - unable to run GPU.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 465
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "source": [
    "sen1 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\r\n",
    "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \" \r\n",
    "\"crushing \"\r\n",
    "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\r\n",
    "\r\n",
    "sen2 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\r\n",
    "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \"\r\n",
    "\"dancing \"\r\n",
    "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\r\n",
    "\r\n",
    "sen3 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\r\n",
    "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \" \r\n",
    "\"run \"\r\n",
    "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\r\n",
    "\r\n",
    "sen4 = (\"Early in the pandemic, there was hope that the \"\r\n",
    "\"cat \"\r\n",
    "\" would one day achieve herd immunity, \"\r\n",
    "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is\"\r\n",
    "\"run \"\r\n",
    "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\r\n",
    "\r\n",
    "sen5 = sen1.split()\r\n",
    "random.shuffle(sen5)\r\n",
    "sen5 = \" \".join(sen5)\r\n",
    "\r\n",
    "sen6 = \" \".join(['Early in the pandemic']*8)\r\n",
    "\r\n",
    "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\r\n",
    "\r\n",
    "for sen in sen_list:\r\n",
    "\r\n",
    "    print(sen)\r\n",
    "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\r\n",
    "        print(sen, file = text_file)\r\n",
    "\r\n",
    "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', text_field=TEXT)\r\n",
    "\r\n",
    "\r\n",
    "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \r\n",
    "                                                  bptt_len=BPTT_LENGTH, repeat=False)\r\n",
    "        \r\n",
    "    sen_loss = evaluate(best_model, sen_iter)\r\n",
    "    print(\"perplexity: \", np.exp(sen_loss))\r\n",
    "    print()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is crushing India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
      "perplexity:  1163.2668983855197\n",
      "\n",
      "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is dancing India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
      "perplexity:  1257.308100991085\n",
      "\n",
      "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is run India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
      "perplexity:  1206.5105081050533\n",
      "\n",
      "Early in the pandemic, there was hope that the cat  would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus isrun India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
      "perplexity:  1657.8267619624894\n",
      "\n",
      "crushing in the world a the countries India there America. over when surging lacks later, easily. pandemic, the coronavirus the the achieve immunity, spread would one hope Latin in and is from with virus year herd hosts Asia second that to was But fearsome Early wave to point a day\n",
      "perplexity:  12980.194336718376\n",
      "\n",
      "Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic\n",
      "perplexity:  55678.85697305307\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extensions\r\n",
    "\r\n",
    "**Task 5** [10]: Extend your model to incorporate the following options: (i) substitute the LSTM with a GRU or a Simple RNN, (ii) increase the number of LSTM layers, (iii) add dropout, (iv) add gradient clipping.  Report on the combination of these options which gives the best performance."
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SzfKRtMBBlA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "source": [
    "'''\r\n",
    "Task 5:\r\n",
    "Small Development Mode:\r\n",
    "Perplexities after running the models with various numbers of layers:\r\n",
    "\r\n",
    "NUM LAYERS\tLSTM\tLSTM\tLSTM sen1-3\tLSTM sen4\tLSTM sen5\tLSTM sen6\tRNN\t    RNN\t    GRU \tGRU \taverage\r\n",
    "2\t        1974\t1784\t1899\t    21.82\t    1627\t    762\t        1881\t1679\t2179\t1942\t1574.882\r\n",
    "3\t        2216\t1974\t2145\t    2552\t    1852\t    816\t        2024\t1788\t1867\t1665\t1889.9\r\n",
    "4\t        1973\t1757\t1863\t    2210\t    1668\t    774\t        1922\t1720\t1971\t1739\t1759.7\r\n",
    "5\t        1862\t1658\t1805\t    2162\t    1625\t    787\t        1970\t1744\t2050\t1807\t1747\r\n",
    "6\t        2208\t1978\t2048\t    2439\t    1773\t    807\t        2097\t1837\t1962\t1734\t1888.3\r\n",
    "\r\n",
    "2 layers appears to be optimal.\r\n",
    "\r\n",
    "Perplexities after running the models with various Dropout values:\r\n",
    "\r\n",
    "Dropout\tLSTM\tLSTM\tLSTM sen1-3\tLSTM sen4\tLSTM sen5\tLSTM sen6\tRNN\t    RNN\t    GRU \tGRU \taverage\r\n",
    "0\t    1908\t1721\t1715\t    1963\t    1573\t    677\t        1872\t1676\t1887\t1688\t1668\r\n",
    "0.25\t1934\t1744\t1767\t    2039\t    1588\t    710\t        1848\t1660\t2105\t1885\t1728\r\n",
    "0.5\t    1974\t1784\t1899\t    2182\t    1627    \t762\t        1881\t1679\t2179\t1942\t1790.9\r\n",
    "0.75\t2069\t1872\t2069\t    2409\t    1718\t    847\t        1973\t1753\t2119\t1909\t1873.8\r\n",
    "1\t    1995\t1847\t2103\t    2440\t    1743\t    997\t        2075\t1860\t2112\t1925\t1909.7\r\n",
    "\r\n",
    "Dropout = 0 appears to be optimal.\r\n",
    "\r\n",
    "Perplexities after running the models with various clipping values:\r\n",
    "\r\n",
    "Grad Clip\tLSTM\tLSTM\tLSTM sen1-3\tLSTM sen4\tLSTM sen5\tLSTM sen6\tRNN\t    RNN\t    GRU \tGRU \taverage\r\n",
    "0\t        1995\t1847\t2103\t    2440\t    1743\t    997\t        2075\t1860\t2112\t1925\t1909.7\r\n",
    "0.5\t        1911\t1722\t1718\t    1968\t    1569\t    681\t        1851\t1661\t1907\t1723\t1671.1\r\n",
    "1\t        1908\t1721\t1715\t    1963\t    1573\t    677\t        1872\t1676\t1927\t1738\t1677\r\n",
    "\r\n",
    "There was not much difference between 0.5 and 1.0 clipping, but both were better than 0 clipping.\r\n",
    "\r\n",
    "I would choose 2 layers, 0 dropout, and 0.5 clipping.\r\n",
    "\r\n",
    "\r\n",
    "Full Version (Using 2 layers, 0 dropout, and 0.5 clipping):\r\n",
    "LSTM\tLSTM\tLSTM sen1   LSTM sen2   LSTM sen3\tLSTM sen4\tLSTM sen5\tLSTM sen6\r\n",
    "377     442     1163        1257        1206        1657        12980       55678\r\n",
    "'''\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nTask 5:\\nPerplexities after running the models with various numbers of layers:\\n\\nNUM LAYERS\\tLSTM\\tLSTM\\tLSTM sen1-3\\tLSTM sen4\\tLSTM sen5\\tLSTM sen6\\tRNN\\t    RNN\\t    GRU \\tGRU \\taverage\\n2\\t        1974\\t1784\\t1899\\t    21.82\\t    1627\\t    762\\t        1881\\t1679\\t2179\\t1942\\t1574.882\\n3\\t        2216\\t1974\\t2145\\t    2552\\t    1852\\t    816\\t        2024\\t1788\\t1867\\t1665\\t1889.9\\n4\\t        1973\\t1757\\t1863\\t    2210\\t    1668\\t    774\\t        1922\\t1720\\t1971\\t1739\\t1759.7\\n5\\t        1862\\t1658\\t1805\\t    2162\\t    1625\\t    787\\t        1970\\t1744\\t2050\\t1807\\t1747\\n6\\t        2208\\t1978\\t2048\\t    2439\\t    1773\\t    807\\t        2097\\t1837\\t1962\\t1734\\t1888.3\\n\\n2 layers appears to be optimal.\\n\\nPerplexities after running the models with various Dropout values:\\n\\nDropout\\tLSTM\\tLSTM\\tLSTM sen1-3\\tLSTM sen4\\tLSTM sen5\\tLSTM sen6\\tRNN\\t    RNN\\t    GRU \\tGRU \\taverage\\n0\\t    1908\\t1721\\t1715\\t    1963\\t    1573\\t    677\\t        1872\\t1676\\t1887\\t1688\\t1668\\n0.25\\t1934\\t1744\\t1767\\t    2039\\t    1588\\t    710\\t        1848\\t1660\\t2105\\t1885\\t1728\\n0.5\\t    1974\\t1784\\t1899\\t    2182\\t    1627    \\t762\\t        1881\\t1679\\t2179\\t1942\\t1790.9\\n0.75\\t2069\\t1872\\t2069\\t    2409\\t    1718\\t    847\\t        1973\\t1753\\t2119\\t1909\\t1873.8\\n1\\t    1995\\t1847\\t2103\\t    2440\\t    1743\\t    997\\t        2075\\t1860\\t2112\\t1925\\t1909.7\\n\\nDropout = 0 appears to be optimal.\\n\\nPerplexities after running the models with various clipping values:\\n\\nGrad Clip\\tLSTM\\tLSTM\\tLSTM sen1-3\\tLSTM sen4\\tLSTM sen5\\tLSTM sen6\\tRNN\\t    RNN\\t    GRU \\tGRU \\taverage\\n0\\t        1995\\t1847\\t2103\\t    2440\\t    1743\\t    997\\t        2075\\t1860\\t2112\\t1925\\t1909.7\\n0.5\\t        1911\\t1722\\t1718\\t    1968\\t    1569\\t    681\\t        1851\\t1661\\t1907\\t1723\\t1671.1\\n1\\t        1908\\t1721\\t1715\\t    1963\\t    1573\\t    677\\t        1872\\t1676\\t1927\\t1738\\t1677\\n\\nThere was not much difference between 0.5 and 1.0 clipping, but both were better than 0 clipping.\\n\\nI would choose 2 layers, 0 dropout, and 0.5 clipping.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 467
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "source": [
    "### RNN ###\r\n",
    "model = RNNLM(\"RNN\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, dropout=DROPOUT)\r\n",
    "if USE_CUDA:\r\n",
    "    model = model.cuda()\r\n",
    "\r\n",
    "loss_fn = nn.CrossEntropyLoss()  # Used instead of NLLLoss.\r\n",
    "learning_rate = 0.001\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "val_losses = []\r\n",
    "best_model = None\r\n",
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    model.train()\r\n",
    "    it = iter(train_iter)\r\n",
    "    hidden = None\r\n",
    "    for i, batch in enumerate(it):\r\n",
    "      text, target = batch.text, batch.target\r\n",
    "      if USE_CUDA:\r\n",
    "          text, target = text.cuda(), target.cuda()\r\n",
    "      hidden = repackage_hidden(hidden)\r\n",
    "      optimizer.zero_grad()\r\n",
    "      yi_hat, hidden = model(text, hidden)  # forward propogation\r\n",
    "      loss = loss_fn(yi_hat.view(-1, yi_hat.size(-1)), target.view(-1))\r\n",
    "      loss.backward()  # backward propogation\r\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\r\n",
    "      optimizer.step()\r\n",
    "      if DEVELOPING:\r\n",
    "        if i % 100 == 0:\r\n",
    "          print(f'At iteration {i} the loss is {loss:.3f}.')\r\n",
    "      else:\r\n",
    "        if i % 1000 == 0:\r\n",
    "          print(f'At iteration {i} the loss is {loss:.3f}.')\r\n",
    "      min_loss = float('inf')\r\n",
    "      if i % 10000 == 0:\r\n",
    "        if loss < min_loss:\r\n",
    "          best_model = type(model)(\"RNN\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, dropout=DROPOUT) \r\n",
    "          best_model.load_state_dict(model.state_dict())\r\n",
    "        val_losses.append(loss)\r\n",
    "\r\n",
    "'''\r\n",
    "Evaluate the loss of best_model on the validation set and compute its perplexity.\r\n",
    "'''\r\n",
    "val_loss = evaluate(best_model, val_iter)\r\n",
    "print(\"perplexity: \", np.exp(val_loss))\r\n",
    "'''\r\n",
    "Evaluate the loss of best_model on the test set and compute its perplexity.\r\n",
    "'''\r\n",
    "test_loss = evaluate(best_model, test_iter)\r\n",
    "print(\"perplexity: \", np.exp(test_loss))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "At iteration 0 the loss is 10.842.\n",
      "At iteration 1000 the loss is 6.556.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-468-5dabd40f7757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m       \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       \u001b[0myi_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# forward propogation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myi_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# backward propogation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-459-dd9d192e101e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden0)\u001b[0m\n\u001b[0;32m     67\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0myi_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0myi_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### GRU ###\r\n",
    "model = RNNLM(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, dropout=DROPOUT)\r\n",
    "if USE_CUDA:\r\n",
    "    model = model.cuda()\r\n",
    "\r\n",
    "loss_fn = nn.CrossEntropyLoss()  # Used instead of NLLLoss.\r\n",
    "learning_rate = 0.001\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "val_losses = []\r\n",
    "best_model = None\r\n",
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    model.train()\r\n",
    "    it = iter(train_iter)\r\n",
    "    hidden = None\r\n",
    "    for i, batch in enumerate(it):\r\n",
    "      text, target = batch.text, batch.target\r\n",
    "      if USE_CUDA:\r\n",
    "          text, target = text.cuda(), target.cuda()\r\n",
    "      hidden = repackage_hidden(hidden)\r\n",
    "      optimizer.zero_grad()\r\n",
    "      yi_hat, hidden = model(text, hidden)  # forward propogation\r\n",
    "      loss = loss_fn(yi_hat.view(-1, yi_hat.size(-1)), target.view(-1))\r\n",
    "      loss.backward()  # backward propogation\r\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\r\n",
    "      optimizer.step()\r\n",
    "      if DEVELOPING:\r\n",
    "        if i % 100 == 0:\r\n",
    "          print(f'At iteration {i} the loss is {loss:.3f}.')\r\n",
    "      else:\r\n",
    "        if i % 1000 == 0:\r\n",
    "          print(f'At iteration {i} the loss is {loss:.3f}.')\r\n",
    "      min_loss = float('inf')\r\n",
    "      if i % 10000 == 0:\r\n",
    "        if loss < min_loss:\r\n",
    "          best_model = type(model)(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, dropout=DROPOUT)\r\n",
    "          best_model.load_state_dict(model.state_dict())\r\n",
    "        val_losses.append(loss)\r\n",
    "\r\n",
    "'''\r\n",
    "Evaluate the loss of best_model on the validation set and compute its perplexity.\r\n",
    "'''\r\n",
    "val_loss = evaluate(best_model, val_iter)\r\n",
    "print(\"perplexity: \", np.exp(val_loss))\r\n",
    "'''\r\n",
    "Evaluate the loss of best_model on the test set and compute its perplexity.\r\n",
    "'''\r\n",
    "test_loss = evaluate(best_model, test_iter)\r\n",
    "print(\"perplexity: \", np.exp(test_loss))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "At iteration 0 the loss is 8.231.\n",
      "At iteration 100 the loss is 7.573.\n",
      "At iteration 200 the loss is 6.492.\n",
      "At iteration 300 the loss is 6.776.\n",
      "At iteration 400 the loss is 5.931.\n",
      "At iteration 500 the loss is 7.974.\n",
      "At iteration 0 the loss is 6.174.\n",
      "At iteration 100 the loss is 6.231.\n",
      "At iteration 200 the loss is 6.105.\n",
      "At iteration 300 the loss is 6.306.\n",
      "At iteration 400 the loss is 5.695.\n",
      "At iteration 500 the loss is 7.796.\n",
      "perplexity:  1907.9382018673427\n",
      "perplexity:  1723.3735961812633\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3-language-model-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}